#create a boxplot of log tranformed data
boxplot(week2.dataset.pos.log)
#create a histogram of log transformed data
hist(week2.dataset.pos.log)
#calculate mean of log transformed data
mean(week2.dataset.pos.log)
#calculate median of log transformed data
median(week2.dataset.pos.log)
#calculate standard deviation of log transformed data
sd(week2.dataset.pos.log)
rm(list=ls())
people<-c("mako", "mika", "mako")
class(people)
factor(people)
as.factor(people)
#factors are like codes for categorical variables
#no difference between factor and as.factor
c(1,2,3)
numbers <- c("1", "2", "3")
numbers
as.numeric(numbers)
tmp<-c(1,2,3, missing)
class(tmp)
tmp<-c(1,2,3, "missing")
class(tmp)
as.numeric(tmp)
#if you don't want numerical data to be treated as numbers, you have to turn it into a factor
as.numeric(people)
rivers
mako.rivers<-list(rivers, rivers*2)
mako.rivers
#lists can contain anything
makorivers[[1]]
mako.rivers[[1]]
mako.rivers[[2]]
names(mako.rivers)<-c("normal.rivers", "big.rivers")
mako.rivers
boxplot(mako.rivers)
# The dollar sign is a shortcut for the double brackets. I.e. [[]] becomes $
mako.rivers$big.rivers
getwd()
setwd("/Users/anissa/Github/uwcom521-anissa")
#PC1.
#Download this dataset in Stata DTA format which contains an anonymized and reduced version
#of the data visualized in the Buechley and Hill paper on Lilypad. Once you have it
#First, found a read.dta() function in a library called foreign
library(foreign)
lilypad <- read.dta("/Users/anissa/Github/uwcom521-anissa/lilypad_anonymized.dta")
#Then take a look at the data to see what's in it:
head(lilypad)
#(a) Reproduce both Table 1 and Table 2 (just US users) using the dataset (as closely as possible).
#Table 1:
table.1 <- table(lilypad$gender, lilypad$order_type)
colnames(table.1) <- c("Arduino", "Both", "LilyPad")
table.1
#Table 2:
#first, make a subset of U.S. customers
us.customers <- subset(lilypad, lilypad$country=="United States")
table.2 <- table(us.customers$gender, us.customers$order_type)
colnames(table.2) <- c("Arduino", "Both", "LilyPad")
table.2
#NOTE: CAN CHANGE THE ORDER BY CHANGING THE LEVELS IN THE DATAFRAME WITH relevel()
#But that seems really complicated. Can also just reassign levels.
#Mako's example with warpbreaks dataset
#factor(wb$tension, levels = c("M", "L", "H"))
#(b) Run a [chi-squared]-test on both tables.
#Compare to the paper (for Table 1, there doesn't seem to be a [chi squared] test for Table 2).
#Did you reproduce it?
chisq.all <- chisq.test(table.1)
chisq.all
chisq.us <- chisq.test(table.2)
chisq.us
#Yes, I reproduced the results of the chi-squared test for Table 1, even though it
#seems to be mislabeled in the paper.
#(c) Install the package "gmodels" and try to display the table using the function CrossTable().
#This will give you output very similar to SPSS.
install.packages("gmodels")
library(gmodels)
CrossTable(table.1)
CrossTable(table.2)
#(c) It's important to be able to import tables directly into your word processor
#without cutting and pasting individual cells. Can you export the output of your table?
#There are a bunch of functions you can use to do this.
#I used the "xtable" package but I think that write.table() and
#Excel would do the job just as well.
write.table(table.1, "/Users/anissa/Github/uwcom521-anissa/LilyPadTable1")
write.table(table.2, "/Users/anissa/Github/uwcom521-anissa/LilyPadTable2")
#Hmmm, that sort of worked. I can't open them in Excel, but I can in a text editor or word
#NOTE: THERE ARE A BUNCH OF R PACKAGES FOR WRITING REGRESSION TABLE OUTPUTS
#E.G. STARGAZER
#PC2.
#At the Community Data Science Workshops we had two parallel afternoon sessions on Day 1.
#In my session, there were 42 participants. In Tommy Guy's session, there were only 19.
#The next week (Day 2), we asked folks to raise their hands if they had been in
#Tommy's session (14 did ) and how many had been in mine (31 did).
#There was clearly attrition in both groups!
#Was there more attrition in one group than another?
#Try answering this both with a test of proportions (prop.test()) and with
#a [chi-squared]. Compare your answers.
#Is there convincing evidence that there is a dependence between instructor and attrition?
#First, I'll make a table so I can visualize what's going on
cscw.attendance <- rbind(c(42, 31),
c(19, 14))
rownames(cscw.attendance) <- c("Mako", "Tommy")
colnames(cscw.attendance) <- c("Day1", "Day2")
cscw.attendance
#Wasn't able to index into cscw.attendance, I think because it's a matrix, so
#going to make it into a dataframe
cscw.attendance <- as.data.frame(cscw.attendance)
class(cscw.attendance)
#Now make a new vector called AttritionRate
#AttritionRate <- c((cscw.attendance$Day1-cscw.attendance)$Day2/cscw.attendance$Day1)
#bind the new vector to the cscw.attendance dataframe
#cscw.attendance <- cbind(cscw.attendance, AttritionRate)
#chisq.test(cscw.attendance)
#I got a warning message, though, sayin that the chi-squared approximation may be incorrect.
#I'm not sure why.
#I shouldn't have added the attrition rate, I think that screwed everything up.
#So commenting out what I did.
chisq.test(cscw.attendance)
prop.test(as.matrix(cscw.attendance))
#Order matters for prop test in a way that it doesn't for Chi Squared. so you can index in.
#E.g. prop.test(cscw.attendance[2], cscw.attendance[1])
#Mako flipped his with the transpose function:
#d(t)
#There appears to be no difference between these.
#I can't figure out how to get the proptest to work.
#NOTE: THAT'S BECAUSE PROPTEST HAS TO BE DONE ON A MATRIX
#PC3.
#Download this dataset that was just published on
#"The Effect of Images of Michelle Obama’s Face on Trick-or-Treaters’ Dietary
#Choices: A Randomized Control Trial."
#The paper doesn't seem to have even been published yet so I think the abstract is all we have.
#We'll come back to it again next week.
#(a) Download and import the data into R. I needed to install the "readstata13" package to do so.
halloween.data <- read.delim("Halloween2012-2014-2015_PLOS.tab")
head(halloween.data)
#I just downloaded the tab delimited version instead of the original Stata version
#(b) Take a look at the codebook if necessary.
#Recode the data on being presented with Michelle Obama's face and
#the data on whether or not kids picked up fruit. we'll leave it at that for now.
#I didn't really understand what we were supposed to recode.
shown.michelle <- subset(halloween.data, halloween.data$obama==1)
not.shown.michelle <- subset(halloween.data, halloween.data$obama==0)
#(c) Do a simple test on whether or not the two groups are dependent.
#Be ready to report those tests. The results in the paper will use linear regression.
#Do you have a sense, from your reading, why your results using the coding material
#we've learned might be different?
#I don't quite understand what test we were supposed to do. t-test?
t.test(shown.michelle$fruit, not.shown.michelle$fruit)
#That does actually work because it's treating them like proportions, but not really
#what we're after
#NOTE: TANYA'S SOLUTION:
# fruitface <- table(d$obama, d$fruit)
# colnames(fruitface) <- c("No", "Yes")
# rownames(fruitface) <- c("No", "Yes")
# fruitface
# going to recreate that:
face.fruit.table <- table(face=halloween.data$obama, fruit=halloween.data$fruit)
colnames(face.fruit.table) <- c("No", "Yes")
rownames(face.fruit.table) <- c("No", "Yes")
face.fruit.table
rm(list = ls())
mako.cars <- mtcars
rm(mako.cars)
mtcars
cor(mtcars$mpg, mcars$hp)
cor(mtcars$mpg, mtcars$hp)
plot(mtcars$mpg, mtcars$hp)
plot(mtcars$mpg, mtcars$hp)
plot(mtcars$mpg, mtcars$hp)
plot(mtcars$hp, mtcars$mpg, method = "spearman")
cor(mtcars$hp, mtcars$mpg, method = "spearman")
lm(mpg ~ hp, data=mtcars)
f <- mpg ~ hp
f
class(f)
lm(f, data=mtcars)
summary(lm(mpg ~ hp, data-mtcars))
summary(lm(mpg ~ hp, data=mtcars))
m <- lm(mpg ~ hp, data=mtcars)
class(m)
summary(m)
m$residuals
m$coefficients
m$fitted.values
coefficients(m)
residuals(m)
predict(m)
confint(m)
confint(m, level=0.99)
summary(m)
-.0682 +c(-1,1) *1.96 * .01012
residuals(m)
hist(residuals(m))
plot(mtcars$hp, residuals(m))
plot(mtcars$hp, residuals(m))
qqnorm(residuals(m))
qqnorm(residuals(m))
d.graph <- data.frame(hp=mtcars$hp, residuals=residual(m))
d.graph <- data.frame(hp=mtcars$hp, residuals=residuals(m))
d.graph
library(ggplot2)
ggplot(data=d.graph + aes(x=hp, y=residuals) + geom_point)
ggplot(data=d.graph) + aes(x=hp, y=residuals) + geom_point())
ggplot(data=d.graph) + aes(x=hp, y=residuals) + geom_point()
f
update.formula(f, . ~ . + disp)
lm(mpg ~ hp + disp, data = mtcars)
summary(lm(mpg ~ hp + disp, data = mtcars))
cor(mtcars$hp, mtcars$disp)
new.d <_ mtcars[,c("hp", "dips", "mpg")]
new.d <- mtcars[,c("hp", "dips", "mpg")]
new.d <- mtcars[,c("hp", "disp", "mpg")]
new.d
cor(new.d)
cor(mtcars)
mako.cars$am <- as.logical(mako.cars$am)
mako.cars <- mtcars
mako.cars$am <- as.logical(mako.cars$am)
head(mako.cars)
summary(lm(mpg ~hp + disp + am, data=mtcars))
summary(lm(mpg ~hp + disp + am, data=mako.cars))
mako.cars
summary(lm(mpg ~hp + disp + am + gear, data=mtcars))
summary(lm(mpg ~hp + disp + am + as.factor(gear), data=mtcars))
mako.cars$gear <- as.factor(mako.cars$gear)
m.full <- lm(mpg ~ hp + disp + am, data=mako.cars)
#Don't save the summary object, but the linear model object
#i.e. "m.full <- summary(lm(mpg ~ hp + disp + am, data=mako.cars))" is wrong
m.disp <- lm(mpg ~ hp + disp, data = mtcars)
m.full
m.disp
install.packages("stargazer")
library(stargazer)
stargazer(m)
#prints in a language called tep which is not what we want, so specify they typ
stargazer(m, type=text)
stargazer(m, m.disp, m.full, type=text)
#Prints a really nice looking table
stargazer(m, type=text)
stargazer(m, m.disp, m.full, type="text")
stargazer(m, m.disp, m.full, type="html")
anova(m.full)
rm(list = ls())
setwd("/Users/anissa/Github/uwcom521-anissa")
list.files()
read.csv("week3_dataset-anissa.csv")
# Load the dataset into a variable called "wk.3.ds"
d.all <- read.csv("week3_dataset-anissa.csv")
head(d.all)
?cor
cor(d.all$x, d.all$y)
?t.test
t.test(d.all$x, d.all$y)
head(d.all)
d.all$i <- as.logical(d.all$i)
d.all
d.all$i <- as.logical(d.all$i)
d.all$j <- as.logical(d.all$j)
head(d.all)
factor(d.all$k.factor, levels=c(0,1,2,3), labels=c("none", "some", "lots", "all"))
lm.x.y <- lm(d.all$x, d.all$y, data=d.all)
lm(mpg ~ hp, data=mtcars)
plot(lm(mpg ~ hp, data=mtcars))
lm(mpg ~ hp, data=mtcars)
plot(lm(mpg ~ hp, data=mtcars))
plot(d.all$x, d.all$y)
lm.x.y <- lm(x ~ y, data=d.all)
lm.x.y <- lm(y~k, data=d.all)
lm.x.y.k <- lm(y ~ x + k, data = d.all)
lm.x.y.k.i.j <- lm(y ~ x + k + i + j, data = d.all)
install.packages("stargazer")
library(stargazer)
stargazer(lm.x.y, lm.x.y.k, lm.x.y.k.i.j, type="text")
lm.x.y <- lm(y ~ x, data=d.all)
lm.x.y.k <- lm(y ~ x + k, data = d.all)
lm.x.y.k.i.j <- lm(y ~ x + k + i + j, data = d.all)
install.packages("stargazer")
library(stargazer)
stargazer(lm.x.y, lm.x.y.k, lm.x.y.k.i.j, type="text")
summary(lm.x.y)
summary(lm.x.y.k)
summary(lm.x.y.k.i.j)
hist(residuals(lm.x.y.k.i.j))
rm(list = ls())
#PC0. Load up your dataset as you did in Week 3 PC2.
setwd("/Users/anissa/Github/uwcom521-anissa")
list.files()
read.csv("week3_dataset-anissa.csv")
# Load the dataset into a variable called "wk.3.ds"
d.all <- read.csv("week3_dataset-anissa.csv")
#PC1.
#If you recall from Week PC6, x and y seemed like they linearly related.
#We now have the tools and terminology to describe this relationship and
#to estimate just how related they are.
#Run a t.test between x and y in the dataset and be ready to interpret the results for the class.
t.test(d.all$x, d.all$y)
#PC2.
#Estimate how correlated x and y are with each other.
cor(d.all$x, d.all$y)
#PC3.
#Recode your data in the way that I laid out in Week 3 PC7.
#Recode i and j as T/F, logicals
d.all$i <- as.logical(d.all$i)
d.all$j <- as.logical(d.all$j)
head(d.all)
#Recode k as a factor and change the numbers into textual "meaning" to make it easier.
#Here's the relevant piece of the codebook (i.e., mapping):
#0=none, 1=some, 2=lots, 3=all.
factor(d.all$k.factor, levels=c(0,1,2,3), labels=c("none", "some", "lots", "all"))
#PC4.
#Generate a set of three linear models and be ready to intrepret the
#coefficients, standard errors, t-statistics, p-values, and R-squared
lm.y.x <- lm(y ~ x, data=d.all)
lm.y.x.k <- lm(y ~ x + k, data = d.all)
lm.y.x.k.i.j <- lm(y ~ x + k + i + j, data = d.all)
summary(lm.y.x)
summary(lm.y.x.k)
summary(lm.y.x.k.i.j)
install.packages("stargazer")
library(stargazer)
stargazer(lm.y.x, lm.y.x.k, lm.y.x.k.i.j, type="text")
hist(residuals(lm.y.x.k.i.j))
plot(d.all$x, residuals(lm.y.x.k.i.j))
plot(d.all$k, residuals(lm.y.x.k.i.j))
plot(d.all$i, residuals(lm.y.x.k.i.j))
plot(d.all$j, residuals(lm.y.x.k.i.j))
class(d.all$k)
d.all$k <- factor(d.all$k.factor, levels=c(0,1,2,3), labels=c("none", "some", "lots", "all"))
class(d.all$j)
d.all$k <- factor(d.all$k, levels=c(0,1,2,3), labels=c("none", "some", "lots", "all"))
head(d.all)
class(d.all$k)
rm(list = ls())
#PC0. Load up your dataset as you did in Week 3 PC2.
setwd("/Users/anissa/Github/uwcom521-anissa")
list.files()
read.csv("week3_dataset-anissa.csv")
# Load the dataset into a variable called "wk.3.ds"
d.all <- read.csv("week3_dataset-anissa.csv")
#PC1.
#If you recall from Week PC6, x and y seemed like they linearly related.
#We now have the tools and terminology to describe this relationship and
#to estimate just how related they are.
#Run a t.test between x and y in the dataset and be ready to interpret the results for the class.
t.test(d.all$x, d.all$y)
#PC2.
#Estimate how correlated x and y are with each other.
cor(d.all$x, d.all$y)
#PC3.
#Recode your data in the way that I laid out in Week 3 PC7.
#Recode i and j as T/F, logicals
d.all$i <- as.logical(d.all$i)
d.all$j <- as.logical(d.all$j)
head(d.all)
#Recode k as a factor and change the numbers into textual "meaning" to make it easier.
#Here's the relevant piece of the codebook (i.e., mapping):
#0=none, 1=some, 2=lots, 3=all.
d.all$k <- factor(d.all$k, levels=c(0,1,2,3), labels=c("none", "some", "lots", "all"))
#PC4.
#Generate a set of three linear models and be ready to intrepret the
#coefficients, standard errors, t-statistics, p-values, and R-squared
lm.y.x <- lm(y ~ x, data=d.all)
lm.y.x.k <- lm(y ~ x + k, data = d.all)
lm.y.x.k.i.j <- lm(y ~ x + k + i + j, data = d.all)
summary(lm.y.x)
summary(lm.y.x.k)
summary(lm.y.x.k.i.j)
install.packages("stargazer")
library(stargazer)
stargazer(lm.y.x, lm.y.x.k, lm.y.x.k.i.j, type="text")
plot(d.all$x, residuals(lm.y.x.k.i.j))
plot(d.all$k, residuals(lm.y.x.k.i.j))
plot(d.all$i, residuals(lm.y.x.k.i.j))
plot(d.all$j, residuals(lm.y.x.k.i.j))
qqnorm(residuals(lm.y.x.k.i.j))
install.packages("stargazer")
library(stargazer)
stargazer(lm.y.x, lm.y.x.k, lm.y.x.k.i.j, type="html")
write(stargazer(lm.y.x, lm.y.x.k, lm.y.x.k.i.j, type="html"), file = "model comparison chart")
write(stargazer(lm.y.x, lm.y.x.k, lm.y.x.k.i.j, type="html"), file = "model.comp.html")
halloween.data <- read.delim("Halloween2012-2014-2015_PLOS.tab")
head(halloween.data)
lm.obama <- lm(fruit ~ obama, data=halloween.data)
summary(lm.obama)
halloween.data$year <- as.factor(halloween.data$year)
lm.obama.controls <- lm(fruit ~ obama + age + year data=halloween.data)
summary(lm.obama.controls)
halloween.data$year <- as.factor(halloween.data$year)
lm.obama.controls <- lm(fruit ~ obama + age + year, data=halloween.data)
summary(lm.obama.controls)
hist(residuals(lm.obama.controls))
plot(halloween.data$obama, residuals(lm.obama.controls))
plot(halloween.data$year, residuals(lm.obama.controls))
plot(halloween.data$age, residuals(lm.obama.controls))
qqnorm(residuals(lm.obama.controls))
load("~/Github/uwcom521-anissa/week_02_dataset-anissa.RData")
class(halloween.data$year)
halloween.data$year
halloween.data.2012 <- subset(halloween.data, year == 2012)
halloween.data.2012
halloween.data.2012 <- subset(halloween.data, year == 2012)
head(halloween.data.2012)
lm.obama <- lm(fruit ~ obama, data=halloween.data.2012)
summary(lm.obama)
halloween.data.2014 <- subset(halloween.data, year == 2014)
head(halloween.data.2014)
lm.obama <- lm(fruit ~ obama, data=halloween.data.2014)
summary(lm.obama)
halloween.data.2015 <- subset(halloween.data, year == 2015)
head(halloween.data.2015)
lm.obama <- lm(fruit ~ obama, data=halloween.data.2015)
summary(lm.obama)
rm(list = ls())
#PC0. Load up your dataset as you did in Week 3 PC2.
setwd("/Users/anissa/Github/uwcom521-anissa")
list.files()
read.csv("week3_dataset-anissa.csv")
# Load the dataset into a variable called "wk.3.ds"
d.all <- read.csv("week3_dataset-anissa.csv")
#PC1.
#If you recall from Week PC6, x and y seemed like they linearly related.
#We now have the tools and terminology to describe this relationship and
#to estimate just how related they are.
#Run a t.test between x and y in the dataset and be ready to interpret the results for the class.
t.test(d.all$x, d.all$y)
#PC2.
#Estimate how correlated x and y are with each other.
cor(d.all$x, d.all$y)
#PC3.
#Recode your data in the way that I laid out in Week 3 PC7.
#Recode i and j as T/F, logicals
d.all$i <- as.logical(d.all$i)
d.all$j <- as.logical(d.all$j)
head(d.all)
#Recode k as a factor and change the numbers into textual "meaning" to make it easier.
#Here's the relevant piece of the codebook (i.e., mapping):
#0=none, 1=some, 2=lots, 3=all.
d.all$k <- factor(d.all$k, levels=c(0,1,2,3), labels=c("none", "some", "lots", "all"))
#PC4.
#Generate a set of three linear models and be ready to intrepret the
#coefficients, standard errors, t-statistics, p-values, and R-squared
lm.y.x <- lm(y ~ x, data=d.all)
lm.y.x.i.j <- lm(y ~ x + i + j, data = d.all)
lm.y.x.i.j.k <- lm(y ~ x + i + j + k, data = d.all)
summary(lm.y.x)
summary(lm.y.x.i.j)
summary(lm.y.x.i.j.k)
#PC5. Generate a set of residual plots for the final model and be ready to
#interpret your model in terms of each of these:
#(a) A histogram of the residuals.
#(b) Plot the residuals by your values of x, i, j, and k (four different plots).
#(c) A QQ plot to evaluate the normality of residuals assumption.
#(a)
hist(residuals(lm.y.x.i.j.k))
#They look pretty normally distributed
#(b)
plot(d.all$x, residuals(lm.y.x.i.j.k))
plot(d.all$k, residuals(lm.y.x.i.j.k))
plot(d.all$i, residuals(lm.y.x.i.j.k))
plot(d.all$j, residuals(lm.y.x.i.j.k))
#I don't undertand why the factor variables are plotted the way they are.
#(c)
qqnorm(residuals(lm.y.x.i.j.k))
#Looks like they're linearly plotted, except for one extreme outlier
#PC6. Generate a nice looking publication-ready table with
#a series of fitted models and put them in a Word document.
install.packages("stargazer")
library(stargazer)
stargazer(lm.y.x, lm.y.x.i.j, lm.y.x.i.j.k, type="html")
write(stargazer(lm.y.x, lm.y.x.i.j, lm.y.x.i.j.k, type="html"), file = "model.comp.html")
#PC7.
#Now, lets go back to the Michelle Obama dataset we used last week
#in the week 7 problem set's programming challenges.
#Load up the dataset once again and fit the following linear models and
#be ready to interpret them similar to the way you did above in PC4
halloween.data <- read.delim("Halloween2012-2014-2015_PLOS.tab")
head(halloween.data)
#(a)
lm.obama <- lm(fruit ~ obama, data=halloween.data)
summary(lm.obama)
#(b)
halloween.data$year <- as.factor(halloween.data$year)
lm.obama.controls <- lm(fruit ~ obama + age + year, data=halloween.data)
summary(lm.obama.controls)
#(c)
hist(residuals(lm.obama.controls))
#Not at all normally distributed. There are two extremes.
plot(halloween.data$obama, residuals(lm.obama.controls))
plot(halloween.data$year, residuals(lm.obama.controls))
plot(halloween.data$age, residuals(lm.obama.controls))
#I'm getting an error message for all of these saying that 'x' and 'y' lengths differ.
#Not sure who that is
#(c)
qqnorm(residuals(lm.obama.controls))
#Those are extremely bifurcated.
#PC9. Run the simple model in (a) three times on three subsets of the dataset:
#just 2012, 2014, and 2015. Be ready to talk through the results.
halloween.data.2012 <- subset(halloween.data, year == 2012)
head(halloween.data.2012)
lm.obama <- lm(fruit ~ obama, data=halloween.data.2012)
summary(lm.obama)
halloween.data.2014 <- subset(halloween.data, year == 2014)
head(halloween.data.2014)
lm.obama <- lm(fruit ~ obama, data=halloween.data.2014)
summary(lm.obama)
halloween.data.2015 <- subset(halloween.data, year == 2015)
head(halloween.data.2015)
lm.obama <- lm(fruit ~ obama, data=halloween.data.2015)
summary(lm.obama)
install.packages("stargazer")
